<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="jp">
<head>
<title>
リバーシプログラムの作り方
</title>
</head>
<body background="../img/bg.gif" bgcolor="#ffffff" text="#000000" link="#0000ff" vlink="#0000ff" alink="#9900ff">
<a href="../index.html">＜目次＞</a>
<a href="8_7.html">＜前へ＞</a>
<a href="bibliography.html">＜次へ＞</a>
<table width="600" border="0" cellpadding="3">
<tr bgcolor="#333399">
<td>
<font size=+1 color="#ffffff">
<b>課題と改善点</b>
</font>
</td>
</tr>
</table>
<p>
本解説では説明しきれなかったこと、今後の改善点について説明します。<br>
</p>
<ul>
<li><b>パターンの変更</b>
<p>
本解説では、評価に比較的少ないマスで構成されるパターンを使用しました。<br>
多くのリバーシプログラムでは、より多いマスで構成されるパターンを使用することが多いです。<br>
例えばLogistelloでは、以下のパターンを使用しています。<br>
</p>
<p>
<img src="../img/pattern_log.gif" width="480" height="400" alt="ロジステロが使用するパターン">
</p>
<p>
ただし、単純にマスの数を多くすればよいというものではありません。<br>
マスの数が多くなれば、それだけ学習させなければならないパターンの数が増えます。<br>
そのため学習にかかる時間が長くなってしまいます。<br>
より強いリバーシプログラムを作成するためにどのようなパターンが適切かを考えてみるのもよいでしょう。<br>
</p>
<li><b>ゲームの段階による評価パラメータの変更</b>
<p>
ゲームの段階とは、局面がどれだけ進んでいるかを表す言葉です。<br>
リバーシでは、単純に現在何手目かで表すことが多いです。<br>
一般に、パターンの評価値は序盤、中盤、終盤によって変わると考えられます。<br>
そのため、ゲームの段階に応じて評価パラメータを使い分けると、より精度の高い評価を行なうことができます。<br>
強いプログラムでは、１局を十数段階に分けて、それぞれ評価パラメータを用意しています。<br>
</p>
<li><b>置換表の改良</b>
<p>
本プログラムで使用した置換表では、古いデータを新しいデータで上書きしていました。<br>
これでは、置換表が頻繁に書き変わる場合には置換表を有効利用できなくなります。<br>
１つのインデックスに対して複数のデータを登録するようにしたり、古いデータを消去する条件を変更する等の置換表の改良を行ってみるとよいでしょう。<br>
</p>
<li><b>探索アルゴリズム</b>
<p>
本解説では探索アルゴリズムにαβ法を使用しましたが、他にも探索アルゴリズム（多くはαβ法の改良ですが）が考案されています。<br>
それらのアルゴリズムを導入してみるとより効率のよい探索を行なえると思います。<br>
どのような探索アルゴリズムがあるかについては各自探してみてください。<br>
</p>
<li><b>定石</b>
<p>
多くの定石を使用できるようにすると、それだけ序盤が安定すると考えられます。<br>
また自動的に定石の学習を行なえるようにしてみるのもよいでしょう。<br>
ただし、本解説の定石クラスでは定石データの検索に線形探索を使用しているため、大量データの検索を高速に行なえません。<br>
大量データの検索を行なえるように、定石データのデータ構造を改良することが必要です。<br>
</p>
<li><b>探索速度の向上</b>
<p>
本解説で記述したプログラムは、強いプログラムと比較すると探索速度がはるかに遅いです。<br>
ここでの探索速度とは、１秒あたりの探索ノード数を指します。<br>
探索速度を向上させるために、着手アルゴリズムを工夫してみるとよいと思います。<br>
リバーシプログラムの中には、ソースコードが公開されているものもあります。<br>
それらを参照すると、どのような工夫を行なっているのか参考になるでしょう。<br>
またプロファイラ等を使用してどこの処理に時間がかかっているのかを調べてみるのもよいでしょう。<br>
</p>
<li><b>学習方法の改善</b>
<p>
本解説では評価パラメータの学習にモンテカルロ法を使用しましたが、学習方法はこれだけではありません。<br>
よくある手法としては、強いプレイヤ同士の棋譜から学習する方法があります。<br>
Buro氏の論文に掲載されているので、興味のある方はご覧ください。<br>
また細かい調整としては以下の点が挙げられます。<br>
<ul>
<li>パラメータ更新を行なう頻度の変更
<li>自己対局時の探索手数変更<br>
探索手数を増やすと、その分質の良い棋譜が得られ、評価関数の質も良くなると予想されます。<br>
<li>ランダムな手として、完全にランダムにするのではなく次善手を選ぶようにする<br>
これも棋譜の質と関連します。<br>
完全にランダムな手では棋譜の質がぐんと落ちますが、次善手なら少し落ちるだけですみます。<br>
</ul>
さらに、強化学習にも様々な手法があります。<br>
興味がある方は専門書を参照してください。<br>
</p>
</ul>
<a href="../index.html">＜目次＞</a>
<a href="8_7.html">＜前へ＞</a>
<a href="bibliography.html">＜次へ＞</a><br>
<hr>
&copy;2006 Daiki Sanno, All Rights Reserved<br>
<a href="mailto:support@es-cube.net">mailto:support@es-cube.net</a><br>
</body>
</html>
